---
title: "Edge vs Serverless for LLM Proxies on Cloudflare Workers"
date: "2025-03-15"
tags: ["Cloudflare", "Edge Computing", "Serverless", "AI"]
summary: "Comparing edge and traditional serverless approaches for building AI API proxies, with real performance data from Hixbi."
---

# Edge vs Serverless for LLM Proxies on Cloudflare Workers

When building Hixbi, I faced a critical architectural decision: should I proxy AI API calls through traditional serverless functions (AWS Lambda, Vercel Functions) or use edge computing with Cloudflare Workers?

## The Challenge

AI APIs like OpenAI, Anthropic, and Google have different response times based on geographic proximity. A user in Tokyo hitting a server in Virginia experiences significantly higher latency than one hitting a nearby edge location.

## Edge Computing: The Winner

After testing both approaches, edge computing with Cloudflare Workers won decisively:

**Performance improvements:**
- TTFB reduced by 30% on average
- P95 latency cut in half for international users
- Zero cold starts (Workers stay warm)

**Cost benefits:**
- 60% cheaper than comparable serverless setup
- No data transfer fees between regions
- Pay only for actual execution time

## Implementation

```javascript
export default {
  async fetch(request, env) {
    const { model, messages } = await request.json()
    
    // Route to nearest AI provider endpoint
    const response = await fetch(getProviderUrl(model), {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${env.API_KEY}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({ messages })
    })
    
    return response
  }
}
```

## Key Insights

1. **Geography matters**: Edge locations reduce round-trip time significantly
2. **Cold starts hurt**: Traditional serverless can have 500ms+ cold starts
3. **Streaming works better**: Edge handles Server-Sent Events more reliably
4. **Cost at scale**: Edge pricing model favors high-traffic applications

## When to Use Each

**Use Edge (Cloudflare Workers) when:**
- You need low latency globally
- Handling real-time streaming responses
- High request volume (>10K/day)
- Want predictable costs

**Use Traditional Serverless when:**
- Complex business logic requiring large dependencies
- Need longer execution times (>30s)
- Existing infrastructure investment
- Regional-only application

## What I'd Do Next

For my next LLM-powered app, I'd start with edge computing from day one. The performance and cost benefits are too significant to ignore, especially for user-facing applications where perceived speed matters.

---
